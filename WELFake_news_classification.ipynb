{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JGzzQlUxURoP",
    "outputId": "0ca348f1-9fed-4604-8a7c-e6fa20c12e9e"
   },
   "outputs": [],
   "source": [
    "! pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fIDt3h4qhbS1",
    "outputId": "21127f87-4156-4653-baa7-f53f0cb9c0b7"
   },
   "outputs": [],
   "source": [
    "! pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FN7vxnx5h9Tu",
    "outputId": "106ab7e6-2eaf-44b0-a198-903e05b113ab"
   },
   "outputs": [],
   "source": [
    "! pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4q77ldF9BT8p",
    "outputId": "417001be-28b9-4674-9adf-716cc308177f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords, words\n",
    "from langdetect import detect\n",
    "from langdetect import detect, DetectorFactory\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "hd6dec2fBfB-",
    "outputId": "bd226f6f-7fcb-437d-d810-66740da135bc"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('WELFake_Dataset.csv', index_col=False)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "cJyPsqZ2B5zj"
   },
   "outputs": [],
   "source": [
    "df.drop([\"Unnamed: 0\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "zKVfyz3RCOOW",
    "outputId": "0eca4c2e-bac4-42f3-8f0f-a955e3fbc185"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum().plot(kind=\"barh\")\n",
    "plt.xlabel('NaN Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toNQyPydDYqc"
   },
   "source": [
    "Above plot shows how much of NaN values present in title and text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQXTzqbhDa9q",
    "outputId": "9ab2c982-7759-43d4-a256-da06a9bcec4d"
   },
   "outputs": [],
   "source": [
    "null = df.isnull().any(axis=1)\n",
    "null_rows = df[null]\n",
    "\n",
    "print(null_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J426SkWwDomp"
   },
   "source": [
    "This shows the total title which have NaN values, which we'll be dropping in next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "TDFF55rrEjOW"
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=['title'], inplace=True)\n",
    "df.dropna(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BCO4sHNq2BC"
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O54-iKmFzAFW",
    "outputId": "2c9c473e-0c19-4d30-b06b-b77a808a1d05"
   },
   "outputs": [],
   "source": [
    "y = df.label\n",
    "print(f'Ratio of real and fake news:')\n",
    "y.value_counts(normalize=True).rename({1: 'real', 0: 'fake'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7cekTnLETH4"
   },
   "source": [
    "Percentage of Real and Fake News:\n",
    "\n",
    "* 51.03% : Real\n",
    "* 48.96% : Fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df['label'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "class_counts.plot(kind='bar', color=['blue', 'green'], alpha=0.7)\n",
    "plt.title('Class Distribution (Fake vs. Real News)', fontsize=16)\n",
    "plt.xlabel('Class', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(rotation=0, fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the chart, the counts of fake and real news appear visually similar, which denotes that there is no class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "LxsJK0zf7mgA",
    "outputId": "f62a8319-7c59-4ff2-b490-a9f6b65e10d8"
   },
   "outputs": [],
   "source": [
    "DetectorFactory.seed = 0\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"Error\"\n",
    "\n",
    "df['language'] = df['text'].apply(detect_language)\n",
    "language_counts = df['language'].value_counts()\n",
    "\n",
    "english_count = language_counts.get('en', 0)\n",
    "non_english_count = sum(language_counts) - english_count\n",
    "\n",
    "labels = 'English', 'Non-English'\n",
    "sizes = [english_count, non_english_count]\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "ax1.axis('equal')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FJ54AUyEt_l"
   },
   "source": [
    "In this pie chart, it shows the amount of English and Non-English text present in 'text' column.\n",
    "\n",
    "From the visualization, it is clear that the non-english content is very small compared to English i.e. 1.9% of the tootal conten. So we can keep neglect this as it'll will not affect our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "a3MZCUdcERq7"
   },
   "outputs": [],
   "source": [
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "BeIbIAEPBvhH",
    "outputId": "fe92e4d7-4dd6-494a-8f05-ff5a30e04c61"
   },
   "outputs": [],
   "source": [
    "def detect_language(title):\n",
    "    try:\n",
    "        return detect(title)\n",
    "    except:\n",
    "        return \"Error\"\n",
    "\n",
    "df['language'] = df['title'].apply(detect_language)\n",
    "language_counts = df['language'].value_counts()\n",
    "\n",
    "english_count = language_counts.get('en', 0)\n",
    "non_english_count = sum(language_counts) - english_count\n",
    "\n",
    "labels = 'English', 'Non-English'\n",
    "sizes = [english_count, non_english_count]\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "ax1.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv2fhuAXGl00"
   },
   "source": [
    "Similar as text column pie chart, the non-english class isn't outweighing the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting punctuations\n",
    "df['punctuation_count'] = df['text'].apply(lambda x: sum([1 for char in x if char in string.punctuation]))\n",
    "\n",
    "# Counting uppercase letters\n",
    "df['uppercase_ratio'] = df['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
    "\n",
    "# Counting numerical data\n",
    "df['numerical_count'] = df['text'].apply(lambda x: sum(1 for char in x if char.isdigit()))\n",
    "\n",
    "# Plot for punctuatios\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='punctuation_count', hue='label', bins=30, kde=True, palette='Set2')\n",
    "plt.title('Punctuation Count Distribution (Fake vs. Real)', fontsize=16)\n",
    "plt.xlabel('Number of Punctuation Marks', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Plot for uppercase letters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='uppercase_ratio', hue='label', bins=30, kde=True, palette='Set2')\n",
    "plt.title('Uppercase Letter Ratio (Fake vs. Real)', fontsize=16)\n",
    "plt.xlabel('Uppercase Letter Ratio', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Plot for numerical data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='numerical_count', hue='label', bins=30, kde=True, palette='Set2')\n",
    "plt.title('Numerical Data Count (Fake vs. Real)', fontsize=16)\n",
    "plt.xlabel('Number of Numerical Characters', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "AplFFQaCKFS8",
    "outputId": "644a862e-e427-44ab-bfcf-6df217361a0b"
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 200, 40)\n",
    "\n",
    "df['title_len'] = df['title'].apply(len) #length of the title\n",
    "df['text_len'] = df['text'].apply(len) #length of the text column\n",
    "df['total_len'] = df['title_len'] + df['text_len'] #total\n",
    "\n",
    "#bins = np.linspace(0, df['total_len'].max(), 40)\n",
    "\n",
    "plt.hist(df[df[\"label\"] == 1][\"total_len\"], bins, alpha=0.5, label=\"Real\", color=\"red\")\n",
    "plt.hist(df[df[\"label\"] == 0][\"total_len\"], bins, alpha=0.5, label=\"Fake\", color=\"green\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('Length of Processed Title')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.title('Distribution of Article Title Lengths by Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RvyfDmHHS9Q"
   },
   "source": [
    "Text length analysis;\n",
    "Comparing the length of fake and real news articles.\n",
    "And we can see there is quite notable difference between real and fake news, real news is a bit longer than fake news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjXKa5DoLdv-"
   },
   "source": [
    "## Visualization of news title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "qoErzhdSKcg4",
    "outputId": "1e6404b2-0a7b-4179-b3c0-68904447621e"
   },
   "outputs": [],
   "source": [
    "titles = ' '.join(title for title in df['title'])\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    max_words=300,\n",
    "    width=800,\n",
    "    height=400,\n",
    ").generate(titles)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4OKVBIrLoxQ"
   },
   "source": [
    "From the above visualization we can infer that most of the news title is related to elections and some of the big leader's names around the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "g1rcmQxcjBXW"
   },
   "outputs": [],
   "source": [
    "# Extracting 2 dataframes from df(dataset) based on the label: 0 & 1 which are fake and real news\n",
    "\n",
    "fake_news = df[df['label'] == 0]\n",
    "real_news = df[df['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "-oyZwfbTs6av",
    "outputId": "073bf848-cbf5-4702-d5c5-a145ce2d39c4"
   },
   "outputs": [],
   "source": [
    "real_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "gHyKmQOikpie",
    "outputId": "9297b99c-55ba-4ffd-b1d4-86b2beaf0201"
   },
   "outputs": [],
   "source": [
    "fake_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_BR0rFYNzUJ"
   },
   "source": [
    "## Visualization of only Fake News of title column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "qgVchXIchq3o",
    "outputId": "eaac140e-311d-4e6f-9a79-867e0150e6cd"
   },
   "outputs": [],
   "source": [
    "fake_texts = ' '.join(text for text in fake_news['title'])\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    max_words=300,\n",
    "    width=800,\n",
    "    height=400,\n",
    ").generate(fake_texts)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwcCBBpYOoIG"
   },
   "source": [
    "## Visualization of only Real News of title_text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "uZCsAKo7nn01",
    "outputId": "76675489-b9f4-4aad-d132-7c70a8f53a4b"
   },
   "outputs": [],
   "source": [
    "real_texts = ' '.join(text for text in real_news['title'])\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    max_words=300,\n",
    "    width=800,\n",
    "    height=400,\n",
    ").generate(real_texts)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract n-grams and their frequencies\n",
    "def get_top_ngrams(corpus, n=2, top_n=20):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english')\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    ngrams = vectorizer.get_feature_names() \n",
    "    counts = X.toarray().sum(axis=0)\n",
    "    ngram_freq = dict(zip(ngrams, counts))\n",
    "    sorted_ngrams = sorted(ngram_freq.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    return sorted_ngrams\n",
    "\n",
    "# Top 20 bigrams and trigrams\n",
    "fake_bigrams = get_top_ngrams(df[df['label'] == 1]['title'], n=2, top_n=20)\n",
    "real_bigrams = get_top_ngrams(df[df['label'] == 0]['title'], n=2, top_n=20)\n",
    "\n",
    "fake_trigrams = get_top_ngrams(df[df['label'] == 1]['title'], n=3, top_n=20)\n",
    "real_trigrams = get_top_ngrams(df[df['label'] == 0]['title'], n=3, top_n=20)\n",
    "\n",
    "print(\"Top 20 Bigrams in Fake News Titles:\")\n",
    "for bigram, freq in fake_bigrams:\n",
    "    print(f\"{bigram}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 20 Bigrams in Real News Titles:\")\n",
    "for bigram, freq in real_bigrams:\n",
    "    print(f\"{bigram}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 20 Trigrams in Fake News Titles:\")\n",
    "for trigram, freq in fake_trigrams:\n",
    "    print(f\"{trigram}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 20 Trigrams in Real News Titles:\")\n",
    "for trigram, freq in real_trigrams:\n",
    "    print(f\"{trigram}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize n-grams\n",
    "def plot_ngrams(ngrams, title):\n",
    "    labels, values = zip(*ngrams)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(labels, values, color='skyblue')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Frequency', fontsize=14)\n",
    "    plt.ylabel('Bigrams', fontsize=14)\n",
    "    plt.gca().invert_yaxis() \n",
    "    plt.show()\n",
    "\n",
    "plot_ngrams(fake_bigrams, 'Top 20 Bigrams in Fake News Titles')\n",
    "plot_ngrams(real_bigrams[:20], 'Top 20 Bigrams in Real News Titles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_trigrams = get_top_ngrams(df[df['label'] == 1]['title'], n=3, top_n=20)\n",
    "real_trigrams = get_top_ngrams(df[df['label'] == 0]['title'], n=3, top_n=20)\n",
    "\n",
    "plot_ngrams(fake_trigrams, 'Top 20 Trigrams in Fake News Titles')\n",
    "plot_ngrams(real_trigrams, 'Top 20 Trigrams in Real News Titles')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment polarity\n",
    "df['sentiment_polarity'] = df['title'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='sentiment_polarity', hue='label', bins=30, kde=True, palette='Set2')\n",
    "plt.title('Sentiment Polarity Distribution (Fake vs. Real News)', fontsize=16)\n",
    "plt.xlabel('Sentiment Polarity', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhP3FVoWrB6O"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ORFuiNoHtxoi"
   },
   "outputs": [],
   "source": [
    "#df.isnull().sum().plot(kind=\"barh\")\n",
    "#plt.show()\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stopWords = set(stopwords.words('english'))\n",
    "        self.englishWords = set(nltk.corpus.words.words())\n",
    "        self.url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "    def url_count(self, text): #Counts URLs\n",
    "        return len(re.findall(self.url_pattern, text))\n",
    "\n",
    "    def remove_urls(self, text): #Remove\n",
    "        return re.sub(self.url_pattern, '', text)\n",
    "\n",
    "    #def to_lowercase(self, text):\n",
    "        #return [token.lower() for token in text]\n",
    "\n",
    "    def remove_contraction(self, text): #Expands contractions\n",
    "        return ' '.join([contractions.fix(word) for word in text.split()])\n",
    "\n",
    "    def clean_tokenize_text(self, text): #Removes non_alphanumeric characters and tokenizes the text into words\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        cleaned_text = re.sub(r',+', ' ', cleaned_text)\n",
    "        return word_tokenize(cleaned_text)\n",
    "\n",
    "    #def stem(self, tokens):\n",
    "        #return [self.stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # def lemmatize(self, tokens): #Reduces each token to its base form using WordNet\n",
    "    #     return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    def pos_lemmatize(self, tokens): # POS-aware lemmatization\n",
    "        pos_tags = nltk.pos_tag(tokens)  # POS tags\n",
    "        return [self.lemmatizer.lemmatize(token, pos=self.get_wordnet_pos(tag)) for token, tag in pos_tags]\n",
    "\n",
    "    def get_wordnet_pos(self, pos_tag): # Map NLTK POS tags to WordNet POS tags\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "        \n",
    "    def handle_negations(self, tokens): #Handling Negations\n",
    "        negations = [\"not\", \"no\", \"never\", \"n't\"]\n",
    "        for i in range(len(tokens) - 1):\n",
    "            if tokens[i] in negations:\n",
    "                tokens[i] = tokens[i] + '_' + tokens[i + 1]\n",
    "                tokens[i + 1] = ''\n",
    "        return [token for token in tokens if token]\n",
    "\n",
    "    def remove_numbers(self, tokens): #Remove Numbers\n",
    "        return [token for token in tokens if not token.isdigit()]\n",
    "    \n",
    "    def remove_special_characters(self, text): #Remove Special Characters\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    def correct_spelling(self, tokens):\n",
    "        return [str(TextBlob(token).correct()) for token in tokens]\n",
    "\n",
    "    def remove_stopWords(self, tokens): #Removes common stopwords\n",
    "        return [w for w in tokens if w not in self.stopWords]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnIN7QQrTMDT"
   },
   "source": [
    "Custom well mannered class which consist of a comprehensive set of tools for text pre-processing, which includes: Removing url, ciontraction, tokenization, lemmatization and removing stopwords.\n",
    "\n",
    "We used lemmatization instead of stemming because stemming just simply removed the only last 3 letters of the token which in our dataset introduced non-english words, as lemmatization reduces the words back to its root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "XBwe5nY2YVAH"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "text_processor = TextProcessor()\n",
    "\n",
    "def process_text_column(column):\n",
    "    tqdm.pandas(desc=\"Processing Articles\")\n",
    "    def process(text):\n",
    "        original_text = text \n",
    "        #print(f\"Original text: {original_text}\")\n",
    "        text = text_processor.remove_urls(text).lower()\n",
    "        #print(f\"After remove_urls: {text}\")\n",
    "\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            print(f\"Skipping due to empty or non-string text: {original_text}\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            # Skip non-English text\n",
    "            if detect(text) != 'en':\n",
    "                print(f\"Skipping non-English text: {original_text}\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error detecting language for text: {original_text}. Error: {e}\")\n",
    "            return []\n",
    "\n",
    "        text = text_processor.remove_contraction(text)\n",
    "        #print(f\"After remove_contraction: {text}\")\n",
    "\n",
    "        text = text_processor.remove_special_characters(text)\n",
    "\n",
    "        tokens = text_processor.clean_tokenize_text(text)\n",
    "        #print(f\"After clean_tokenize_text: {tokens}\")\n",
    "       \n",
    "        tokens = text_processor.remove_numbers(tokens)\n",
    "\n",
    "        tokens = text_processor.handle_negations(tokens)\n",
    "\n",
    "        tokens = text_processor.remove_stopWords(tokens)\n",
    "        #print(f\"After remove_stopWords: {tokens}\")\n",
    "\n",
    "        tokens = text_processor.correct_spelling(tokens)\n",
    "\n",
    "        #tokens = text_processor.stem(tokens)\n",
    "        #print(f\"After stem: {tokens}\")\n",
    "\n",
    "        tokens = text_processor.pos_lemmatize(tokens)\n",
    "\n",
    "        #tokens = text_processor.lemmatize(tokens)\n",
    "        print(f\"After lemmatize: {tokens}\") \n",
    "        return tokens\n",
    "\n",
    "    return column.progress_apply(process)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxyoGrzzVmeH"
   },
   "source": [
    "Defined a function which processes text data column-wise, as it is made to work with our custom 'TextProcessor' class. It consist of some debug code which when executed gives all the modifications our dataset will go through while doing this pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "FnGvvyYBUPxq"
   },
   "outputs": [],
   "source": [
    "#df_sampled = df.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0LaE0mhpYgpN",
    "outputId": "5b487b90-1b85-4bc5-a280-fa645587b40f"
   },
   "outputs": [],
   "source": [
    "df['processed_title'] = process_text_column(df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "HHP3SqycZc6T",
    "outputId": "190f819d-d433-415d-99b7-a8ffd7554797"
   },
   "outputs": [],
   "source": [
    "df['processed_text'] = process_text_column(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwN5-r9QcNJu",
    "outputId": "a82613ad-2129-462b-d6cd-02bc2203bf68"
   },
   "outputs": [],
   "source": [
    "df['processed_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('preprocessed_welfake.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('FINAL_FINAL_FINALLLLLLLLLLLLLL_preprocessed_welfake.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Relevant features for the model\n",
    "feature_columns = [\n",
    "    'punctuation_count', 'uppercase_ratio', 'numerical_count',\n",
    "    'sentiment_polarity', 'title_len', 'text_len', 'total_len'\n",
    "]\n",
    "X = df[feature_columns]\n",
    "y = df['label'] \n",
    "\n",
    "# DataSplit\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, log_loss\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, lr.predict_proba(X_test_scaled)[:, 1]))\n",
    "\n",
    "lr_acc = accuracy_score(y_test, y_pred_lr)\n",
    "lr_loss = log_loss(y_test, lr.predict_proba(X_test_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(probability=True, random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_svm = svm.predict(X_test_scaled)\n",
    "print(\"SVM Performance:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, svm.predict_proba(X_test_scaled)[:, 1]))\n",
    "\n",
    "svm_acc = accuracy_score(y_test, y_pred_svm)\n",
    "svm_loss = log_loss(y_test, svm.predict_proba(X_test_scaled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Random Forest Performance:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "rf_acc = accuracy_score(y_test, y_pred_rf)\n",
    "rf_loss = log_loss(y_test, rf.predict_proba(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_xgb = xgb.predict(X_test_scaled)\n",
    "print(\"XGBoost Performance:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, xgb.predict_proba(X_test_scaled)[:, 1]))\n",
    "\n",
    "xgb_acc = accuracy_score(y_test, y_pred_xgb)\n",
    "xgb_loss = log_loss(y_test, xgb.predict_proba(X_test_scaled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "df['combined_processed_text'] = df['processed_title'] + df['processed_text']\n",
    "\n",
    "# vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df['combined_processed_text'])\n",
    "\n",
    "# use numerical features only\n",
    "numerical_features = ['punctuation_count', 'uppercase_ratio', 'numerical_count', \n",
    "                      'sentiment_polarity', 'text_len', 'title_len', 'total_len']\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# combine TF-IDF and scaled num features\n",
    "feature_matrix = hstack([tfidf_features, scaled_features])\n",
    "\n",
    "X = feature_matrix\n",
    "y = df['label'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "rf_test_pred = rf.predict(X_test)\n",
    "\n",
    "rf_test_pred_ensemble = rf.predict_proba(X_test)\n",
    "rf_train_pred_ensemble = rf.predict_proba(X_train)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rf_test_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, rf_test_pred))\n",
    "\n",
    "rf_tfidf_acc = accuracy_score(y_test, rf_test_pred)\n",
    "rf_tfidf_loss = log_loss(y_test, rf_test_pred_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(\n",
    "    n_estimators=100, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=6, \n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "xgb_test_pred = xgb.predict(X_test)\n",
    "\n",
    "xgb_test_pred_ensemble = xgb.predict_proba(X_test)\n",
    "xgb_train_pred_ensemble = xgb.predict_proba(X_train)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, xgb_test_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, xgb_test_pred))\n",
    "\n",
    "xgb_tfidf_acc = accuracy_score(y_test, xgb_test_pred)\n",
    "xgb_tfidf_loss = log_loss(y_test, xgb_test_pred_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['combined_processed_text'].apply(lambda x: ' '.join(x))  # join tokens into sentences\n",
    "labels = df['label']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_length = 200\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_shape=(max_length,)),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_test_probs = model.predict(X_test)\n",
    "lstm_train_probs = model.predict(X_train)\n",
    "\n",
    "lstm_test_pred = (lstm_test_probs > 0.5).astype(\"int32\")\n",
    "lstm_train_pred = (lstm_train_probs > 0.5).astype(\"int32\")\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, lstm_test_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, lstm_test_pred))\n",
    "\n",
    "lstm_acc = accuracy_score(y_test, lstm_test_pred)\n",
    "lstm_loss = log_loss(y_test, lstm_test_probs)\n",
    "\n",
    "print(f\"Loss: {lstm_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stack = np.hstack([rf_train_pred_ensemble , xgb_train_pred_ensemble, lstm_train_pred])\n",
    "test_stack = np.hstack([rf_test_pred_ensemble , xgb_test_pred_ensemble, lstm_test_pred])\n",
    "\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(train_stack, y_train)\n",
    "\n",
    "ensemble_preds = meta_model.predict(test_stack)\n",
    "\n",
    "print(\"Ensemble Model Accuracy:\", accuracy_score(y_test, ensemble_preds))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, ensemble_preds))\n",
    "\n",
    "ensemble1_acc = accuracy_score(y_test, ensemble_preds)\n",
    "ensemble1_loss = log_loss(y_test, meta_model.predict_proba(test_stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model - 2 (Weighted Averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_probs = rf_test_pred_ensemble \n",
    "xgb_probs = xgb_test_pred_ensemble\n",
    "lstm_probs = lstm_test_pred \n",
    "\n",
    "# weights\n",
    "rf_weight = 0.6 \n",
    "xgb_weight = 0.2\n",
    "lstm_weight = 0.1\n",
    "\n",
    "ensemble_probs = (\n",
    "    rf_weight * rf_probs + xgb_weight * xgb_probs + lstm_weight * lstm_probs\n",
    ")\n",
    "\n",
    "ensemble_preds = np.argmax(ensemble_probs, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, ensemble_preds)\n",
    "print(\"Ensemble Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, ensemble_preds))\n",
    "\n",
    "ensemble2_acc = accuracy_score(y_test, ensemble_preds)\n",
    "ensemble2_loss = log_loss(y_test, meta_model.predict_proba(test_stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined_text_title'] = df['title'] + df['text']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "texts = df['combined_text_title'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_dataset = NewsDataset(X_train, y_train, tokenizer, max_len)\n",
    "val_dataset = NewsDataset(X_val, y_val, tokenizer, max_len)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # Binary classification\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, data_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += (torch.argmax(logits, dim=1) == labels).sum().item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    accuracy = total_correct / len(data_loader.dataset)\n",
    "    return total_loss / len(data_loader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, loss_fn, device, save_logits=False, logits_file=\"bert_logits.npy\"):\n",
    "    model.eval()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    all_logits = []\n",
    "\n",
    "    progress_bar = tqdm(data_loader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (torch.argmax(logits, dim=1) == labels).sum().item()\n",
    "\n",
    "            if save_logits:\n",
    "                all_logits.append(logits.cpu().numpy())\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    if save_logits:\n",
    "        all_logits = np.vstack(all_logits)\n",
    "        np.save(logits_file, all_logits)\n",
    "        print(f\"Logits saved to '{logits_file}'\")\n",
    "\n",
    "    accuracy = total_correct / len(data_loader.dataset)\n",
    "    return total_loss / len(data_loader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    train_loss, train_acc = train_model(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss, val_acc = evaluate_model(model, val_loader, loss_fn, device)\n",
    "\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds, y_true = [], []\n",
    "bert_acc = 0.0\n",
    "bert_loss = float('inf')\n",
    "total_loss = 0.0 \n",
    "total_correct = 0  \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += (torch.argmax(logits, dim=1) == labels).sum().item()\n",
    "\n",
    "        y_preds.extend(torch.argmax(logits, dim=1).tolist())\n",
    "        y_true.extend(labels.tolist())\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = total_loss / len(val_loader)\n",
    "val_acc = total_correct / len(val_loader.dataset)\n",
    "\n",
    "if val_acc > bert_acc:\n",
    "    bert_acc = val_acc\n",
    "if val_loss < bert_loss:\n",
    "    bert_loss = val_loss\n",
    "\n",
    "bert_acc = val_acc\n",
    "bert_loss = val_loss\n",
    "\n",
    "print(f'Best Validation Accuracy: {bert_acc:.4f}')\n",
    "print(f'Best Validation Loss: {bert_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = min(len(bert_probs), len(lstm_probs), len(y_test))\n",
    "\n",
    "# Resize\n",
    "bert_probs = bert_probs[:min_len]\n",
    "lstm_probs = lstm_probs[:min_len]\n",
    "y_test = y_test[:min_len]\n",
    "\n",
    "# Weights\n",
    "bert_weight = 0.9\n",
    "lstm_weight = 0.1\n",
    "\n",
    "ensemble_probs_bert_lstm = bert_weight * bert_probs + lstm_weight * lstm_probs\n",
    "ensemble_preds_bert_lstm = (ensemble_probs_bert_lstm > 0.5).astype(\"int32\")\n",
    "\n",
    "ensemble3_acc = accuracy_score(y_test, ensemble_preds_bert_lstm)\n",
    "ensemble3_loss = log_loss(y_test, ensemble_probs_bert_lstm)\n",
    "\n",
    "print(\"BERT + LSTM Ensemble Accuracy:\", ensemble3_acc)\n",
    "print(\"BERT + LSTM Ensemble Loss:\", ensemble3_loss)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, ensemble_preds_bert_lstm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble model - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = min(len(bert_probs), len(lstm_probs), len(rf_probs), len(xgb_probs), len(y_test))\n",
    "\n",
    "bert_probs = bert_probs[:min_len]\n",
    "lstm_probs = lstm_probs[:min_len]\n",
    "rf_probs = rf_probs[:min_len]\n",
    "xgb_probs = xgb_probs[:min_len]\n",
    "y_test = y_test[:min_len]\n",
    "\n",
    "# Weights\n",
    "bert_weight = 0.7\n",
    "lstm_weight = 0.1\n",
    "rf_weight = 0.1\n",
    "xgb_weight = 0.1\n",
    "\n",
    "ensemble_probs_all = (\n",
    "    bert_weight * bert_probs +\n",
    "    lstm_weight * lstm_probs +\n",
    "    rf_weight * rf_probs +\n",
    "    xgb_weight * xgb_probs\n",
    ")\n",
    "\n",
    "ensemble_preds_all = (ensemble_probs_all > 0.5).astype(\"int32\")\n",
    "\n",
    "ensemble4_acc = accuracy_score(y_test, ensemble_preds_all)\n",
    "ensemble4_loss = log_loss(y_test, ensemble_probs_all)\n",
    "\n",
    "print(\"BERT + LSTM + Random Forest + XGBoost Ensemble Accuracy:\", ensemble4_acc)\n",
    "print(\"BERT + LSTM + Random Forest + XGBoost Ensemble Loss:\", ensemble4_loss)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, ensemble_preds_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_names = ['Logistic Regression', 'SVM', 'Random Forest', 'XGBoost', \n",
    "               'RF TF-IDF', 'XGB TF-IDF', 'LSTM', 'Ensemble 1', 'Ensemble 2', \n",
    "               'BERT', 'Ensemble 3', 'Ensemble 4']\n",
    "\n",
    "accuracies = [\n",
    "    lr_acc, svm_acc, rf_acc, xgb_acc,\n",
    "    rf_tfidf_acc, xgb_tfidf_acc, lstm_acc, ensemble1_acc, \n",
    "    ensemble2_acc, bert_acc, ensemble3_acc, ensemble4_acc\n",
    "]\n",
    "\n",
    "losses = [\n",
    "    lr_loss, svm_loss, rf_loss, xgb_loss,\n",
    "    rf_tfidf_loss, xgb_tfidf_loss, lstm_loss, ensemble1_loss, \n",
    "    ensemble2_loss, bert_loss, ensemble3_loss, ensemble4_loss\n",
    "]\n",
    "\n",
    "accuracies_percentage = [acc * 100 for acc in accuracies]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(model_names, accuracies_percentage, color='skyblue')\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.title('Model Accuracies (in %)')\n",
    "plt.xlim(0, 100)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(model_names, losses, color='lightcoral')\n",
    "plt.xlabel('Log Loss')\n",
    "plt.title('Model Losses')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_preprocessed_welfake_dataset.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "everythingmlai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
